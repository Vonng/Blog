# 神经网络优化

[author]: # "Vonng (fengruohang@outlook.com)"
[tags]: # "神经网络，数学"
[category]: # "math/nndl"
[mtime]: #	"2017-05-11 10:00 "
神经网络基础进阶，优化手段导论

---





神经网络的基础知识不多，但优化其表现却是一个无尽的挑战。每一种优化的手段都可以当做一个进阶的课题深入研究。改进神经网络的学习效果有几种主要的方法：

- 选取**更好的代价函数**：例如**交叉熵（cross-entropy）**
- 修改神经网络的输出层：**柔性最大值(softmax)**
- **规范化（regularization）**：**L2规范化**、弃权、L1规范化
- 人为扩展已有训练数据集
- 使用更好的**初始化权重**
- 采用其他的**激活神经元**：线性修正神经元（ReLU），双曲正切神经元（tansig）
- 修改神经网络输入的组织方式：递归神经网络（Recurrent NN），卷积神经网络（Convolutional NN）。
- 添加层数：深度神经网络（Deep NN）
- 通过尝试，选择合适的**超参数（hyper-parameters）**，按照迭代轮数或评估效果动态调整超参数。
- 采用其他的梯度下降方法：基于动量的梯度下降





## 交叉熵代价函数

MSE是一个不错的代价函数，然而它存在一个很尴尬的问题：学习速度。

MSE输出层误差的计算公式为：
$$
δ^L =  (a^L - y)σ'(z^L)
$$
sigmoid又称为逻辑斯蒂曲线，其导数$σ'$是一个钟形曲线。所以当带权输入$z$从大到小或从小到大时，梯度的变化会经历一个“小，大，小”的过程。学习的速度也会被导数项拖累，存在一个“慢，快，慢”的过程。

| MSE                 | Cross Entropy                 |
| ------------------- | ----------------------------- |
| ![](images/mse.png) | ![](images/cross-entropy.png) |

若采用**交叉熵（cross entropy）**误差函数：
$$
C = - \frac 1 n \sum_x [ y ln(a) + (1-y)ln(1-a)]
$$
对于单个样本，即
$$
C = - [ y ln(a) + (1-y)ln(1-a)]
$$
虽然看起来很复杂，但输出层的误差公式变得异常简单，变为：$δ^L =  a^L - y$ 

比起MSE少掉了导数因子，所以误差直接和（预测值-实际值）成正比，不会遇到学习速度被激活函数的导数拖慢的问题，计算起来也更为简单。

#### 证明

$C$对网络输出值$a$求导，则有：

$$
∇C_a = \frac {∂C} {∂a^L} = - [ \frac y a - \frac {(1-y)} {1-a}] = \frac {a - y} {a (1-a)}
$$

反向传播的四个基本方程里，与误差函数$C$相关的只有BP1：即输出层误差的计算方式。

$$
δ^L = ∇C_a ⊙ σ'(z^L)
$$

现在$C$换了计算方式，将新的误差函数$C$对输出值$a^L$的梯度$\frac {∂C} {∂a^L}$带回BP1，即有：

$$
δ^L = \frac {a - y} {a (1-a)}× a(1-a) = a-y
$$

#### 意义

对于一个随机分布$p$，如果使用针对分布$p$优化的编码$L=-log\,p(x)$，则平均的消息长度为
$$
H(p) = - \sum_{x}{p(x)\ log\,{p(x)}}
$$
这时可以证明，平均编码长度是最优的。但如果使用了针对另一个随机分布$q$优化的编码$L = -log\,q(x)$，就会出现一定程度的无效性。这时候消息的平均长度就需要$H_q(p)$个比特来表示。
$$
H_q(p) =- \sum_{x} p(x)\ log\,q(x)
$$
这里，实际分布是$p$，但编码为字母$x$设置的代价$-log\,q(x)$却是按照分布$q$进行优化的。

**$H_q(p)​$称为分布$p​$相对于分布$q​$的交叉熵（cross entropy）**，它衡量了为分布$q​$优化的编码，在表述分布$p​$时的平均消息长度。交叉熵越大，说明两个分布的距离也越大。

而信息论中的交叉熵又是如何和神经网络中的交叉熵代价函数联系起来的呢？
$$
C = - [ y ln(a) + (1-y)ln(1-a)]
$$
如果我们将$a$看做估计神经元$y=1$的概率，将$(1-a)$看做估计神经元$y=0$的概率。从形式上代价函数就可以表示为**实际结果$y$分布对预测结果$a$的交叉熵**，神经网络的参数是为输出$a$优化的，如果希望其输出真正的结果$y$，这个代价就可以用实际分布$y$，对预测分布$a$的交叉熵来衡量。



## 柔性最大值输出层与对数似然损失函数

**柔性最大值（softmax）**是一种新式的神经网络输出层，在神经网络中有着广泛的应用。

柔性最大值层和普通的层类似，都会首先计算带权输入：$z = Wx+b$，但不同的是，柔性最大值层不会应用$a=σ(z)$来产生输出的激活值。而是使用柔性最大值函数：
$$
a_j^L = \frac{ e^{z_j^L}}{\sum_k{e^{z^L_k}}}
$$
简单的说，它把所有带权输入$z$先喂进指数函数$e^x$里，然后对每个神经元所得值进行和归一化。

因为概率分布是归一化的，所以如果一个用于分类的神经网络在输出层对所有输出进行和归一化处理，那么输出自然而然变为结果分类的**概率分布**。

#### 性质

柔性最大值函数是单调的，所以某个神经元的带权输入越大，相应的激活值也越大。

柔性最大值是非局部的，每一个神经元的输出激活值都依赖本层所有的带权输入$z$。

柔性最大值往往和对数似然代价函数配合使用，这样BP1中的$∇C_a$就比较好求了。

### 对数似然函数（log-likelihood）

对数似然函数是配合柔性最大值使用的代价函数：
$$
C = - \sum_j (y_j \ln a_j)
$$

该函数背后的直觉是这样的：柔性最大值给出了分类概率分布，而样本则给出了正确的分类$y$。那么$a^L_y$就是将样本输入$x$正确分类的概率，位于0~1之间。相应的$-ln\,a^L_y$给出了对应的代价，当概率为$0$时代价无穷大，而概率为1时代价为$0$。之所以使用对数似然代价函数，而不继续使用交叉熵函数，是因为 log-likelihodd + softmax 恰好与 cross-entropy有着同样的输出层误差表达式。即：
$$
δ^L = a^L - y
$$

#### 证明

因为代价函数$C = - \sum_j(y_j \ln a_j)$，则易得代价函数对激活值的梯度为：$\frac{∂C}{∂a_k}  = - \frac {y_k}{a_k}$。

因为softmax层是非局部的，每一个激活值都和所有的带权输入有关联，所以有：
$$
δ^L =  \frac{∂C}{∂z_j} 
= \sum_{k} \frac{∂C}{∂a_k} \frac{∂a_k}{∂z_j}
= -  \sum_{k} \frac {y_k}{a_k} \frac{∂a_k}{∂z_j}
= - \left[\frac{y_j}{a_j} \frac{∂a_j}{∂z_j}  + \sum_{j\ne k}{ \frac {y_k}{a_k} \frac{∂a_k}{∂z_j}} \right]
$$
这里需要对下标$j,k$分情况讨论，首先为了求导方便可令$u=e^{z_j}。$当$k=j$时：
$$
\begin{align}
\frac{∂a_k}{∂z_j} &=  \frac{∂a_j}{∂u} \frac{∂u}{∂z_j}  = \left[\frac{ u}{e^{z_0}+e^{z_1}+\cdots+[u]+\cdots+e^{z_{d_L}}}\right]'e^{z_j}\\
& =\frac{ (\sum_i{e^{z_i}}) - e^{z_j} }{(\sum_i{e^{z_i}})^2} e^{z_j}
= \left(\frac {\sum_i{e^{z_i}}}{\sum_i{e^{z_i}}} - \frac{  e^{z_j} }{\sum_i{e^{z_i}}}\right)  \frac{e^{z_j}}{\sum_i{e^{z_i}}}
= (1 - a_j)a_j
\end{align}
$$
而当$k \ne j$时，则有：
$$
\begin{align}
\frac{∂a_k}{∂z_j} &=  \frac{∂a_k}{∂u} \frac{∂u}{∂z_j}  = \left[\frac{ e^{z_k}}{e^{z_0}+e^{z_1}+\cdots+[u]+\cdots+e^{z_{d_L}}}\right]'e^{z_j}\\
& = \frac{0 - e^{z_k} }{(\sum_i{e^{z_i}})^2} e^{z_j} = - \frac{e^{z_k}}{\sum_i{e^{z_i}}} \frac{e^{z_j}}{\sum_i{e^{z_i}}}
= - a_ka_j
\end{align}
$$
回带有：
$$
\frac{∂C}{∂z_j}  = - \left[\frac{y_j}{a_j} \frac{∂a_j}{∂z_j}  + \sum_{j\ne k}{ \frac {y_k}{a_k} \frac{∂a_k}{∂z_j}} \right]  
= \left[    y_j(a_j - 1) +   \sum_{j\ne k} a_j y_k \right] 
= \left[ a_j  \left( y_j + \sum_{k\ne j} y_k\right) - y_j \right] 
= (a_j - y_j)
$$


QED

所以结论是：一个具有对数似然代价的柔性最大层，和交叉熵代价的S型输出层是相似的。



## 规范化

拥有大量的自由参数的模型能够描述特别神奇的现象。

费米说："With four parameters I can fit an elephant, and with five I can make him wiggle his trunk"。神经网络这种动辄百万的参数的模型能拟合出什么奇妙的东西是难以想象的。

一个模型能够很好的拟合已有的数据，可能只是因为模型中足够的自由度，使得它可以描述几乎所有给定大小的数据集，而不是真正洞察数据集背后的本质。发生这种情形时，**模型对已有的数据表现的很好，但是对新的数据很难泛化**。这种情况称为**过拟合（overfitting）**。

例如用3阶多项式拟合一个带随机噪声的正弦函数，看上去就还不错；而10阶多项式，虽然完美拟合了数据集中的所有点，但实际预测能力就很离谱了。它拟合的更多地是数据集中的噪声，而非数据集背后的潜在规律。

```python
x, xs = np.linspace(0, 2 * np.pi, 10), np.arange(0, 2 * np.pi, 0.001)
y = np.sin(x) + np.random.randn(10) * 0.4
p1,p2 = np.polyfit(x, y, 10), np.polyfit(x, y, 3)
plt.plot(xs, np.polyval(p1, xs));plt.plot(x, y, 'ro');plt.plot(xs, np.sin(xs), 'r--')
plt.plot(xs, np.polyval(p2, xs));plt.plot(x, y, 'ro');plt.plot(xs, np.sin(xs), 'r--')
```

| 3阶多项式                     | 10阶多项式                     |
| ------------------------- | -------------------------- |
| ![](images/overfit-3.png) | ![](images/overfit-10.png) |

一个模型真正的测验标准，是它对没有见过的场景的预测能力，称为**泛化能力（generalize）**。

如何避免过拟合？按照奥卡姆剃刀原理：**因不宜超出果之需**，或者用其它的方式来表述：**最简单的解释是最佳的。**

当然这个原理只是我们抱有的一种信念，并不是真正的定理铁律：这些数据点真的由拟合出的十阶多项式产生，也不能否认这种可能…

总之，如果出现非常大的权重参数，通常就意味着过拟合。例如拟合所得十阶多项式系数就非常畸形：

```
 -0.001278386964370502
 0.02826407452052734
 -0.20310716176300195
 0.049178327509096835
 7.376259706365357
 -46.295365250182925
 135.58265224859255
 -211.767050023543
 167.26204130954324
 -50.95259728945658
 0.4211227089756039
```

### L2规范化

通过添加权重衰减项，可以有效遏制过拟合。例如$L2$规范化为损失函数添加了一个$\frac λ 2 w^2$的惩罚项：
$$
C = -\frac{1}{n} \sum_{xj} \left[ y_j \ln a^L_j+(1-y_j) \ln
  (1-a^L_j)\right] + \frac{\lambda}{2n} \sum_w w^2
$$

所以，权重越大，损失值越大，这就避免神经网络了向拟合出畸形参数的方向发展。

这里使用的是交叉熵损失函数。但无论哪种损失函数，都可以写成：

$$
C = C_0 + \frac {λ}{2n} \sum_w {w^2}
$$

其中原始的代价函数为$C_0$。那么，原来损失函数对权值的偏导，就可以写成：

$$
\frac{∂C}{∂w} = \frac{ ∂C_0}{∂w}+\frac{λ}{n} w
$$

因此，引入$L2$规范化惩罚项在计算上的唯一变化，就是在处理权值梯度时首先要乘一个衰减系数：

$$
w → w' = w\left(1 - \frac{ηλ}{n} \right) - η\frac{∂C_0}{∂ w}
$$

注意这里的$n$是所有的训练样本数，而不是一个小批次使用的训练样本数。

通常，规范化只对权重而不对偏置进行。因为实践上不对偏置进行规范化是一种习惯：**一个大的偏置并不会像大的权重那样让神经元对输入太过敏感。**

### 其他规范化：L1规范化

相比L2规范化的代价函数$C = C_0 + \frac {λ}{2n} \sum_w {w^2}$，L1的主要区别就是惩罚项是一阶的：
$$
C = C_0 + \frac {λ}{n} \sum_w |w|
$$
L2规范化，实际上每次让权重按比例进行一次缩减，而L1规范化的惩罚项对权值求导之后变为常数项，实际上是每次让权值以一个常数值进行缩减。所以，L1规范化会使权重小的连接快速萎缩，倾向于将网络的权重聚集于少数高重要度的连接上。

### 其他规范化：弃权(Drop connect)

弃权使用了这样一种方式：每次使用训练数据训练网络时，随机（临时）地从网络中去除一部分隐藏神经元。比如，每次隐藏掉一半的神经元，然后在全部参与一次前馈的过程中，将这些隐藏神经元的输出权值减半。

弃权有点类似在训练不同的神经网络，然后取效果的平均。同时弃权减少了复杂的相互适应的神经元，使得模型对一部分证据的丢失也能保持健壮。



## 扩展训练数据集

实际上影响训练效果的核心因素是样本数量。对于MNIST，可以通过人为地轻微调整图片，有效地扩展训练集的规模。也可以通过添加噪声的方式来扩展训练集

现在MNIST效果最好的网络之一没有用什么特殊的优化方法，仅仅通过扩展训练数据集就达到了99.6%的准确率。



## 更好的权重初始化方法

随机初始化权重并不是最好的初始化方式。

如果每个参数都使用标准正态分布进行初始化，比如500个输入权值一个偏置。那么带权输入$z$的分布就是一个均值为0，标准差为$\sqrt{501} = 22.4$的高斯分布。这样的分布很不好，很容易产生特别大特别小的带权输入，让神经元过于压抑或兴奋。

我们其实是希望**神经元的激活值**差不多满足标准正态分布，而不是某个权值或偏置。因此为了实现这个目标，让每个权值初始化时使用标准差为$\sqrt{n_{in}}$的正态分布，而不是标准正态分布，就可以使得激活值有这样的性质。



## 超参数的选取

### 学习速率

当网络趋于收敛时，如果学习速率$η$太大，步子迈得太大容易扯到蛋，就容易走过头。

估计$η$没什么特别好的办法，基本就是不断尝试了，先从数量级开始试起。

#### 超参数的超参数

学习速率也并不一定非要一成不变，也许使用一个迭代次数和准确率的衰减函数来动态调整学习速率，是一个不错的选择。例如：每十回合效果不提升时学习速率减半。

### 小批量数据大小

实际上一次只用一个样本做梯度下降也不是太糟糕的注意，但小批量数据大小$m$的设置还需要考虑其他因素：

小批量数据尺寸太小，没法充分利用矩阵库的批量计算能力。小批量数据尺寸太大，更新参数的频率会下降。所以也需要进行一个折仲。通常取10就可以。



## 其他激活函数：双曲正切神经元

双曲正切神经元的激活函数是：
$$
tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
$$
其实双曲正切和S型神经元师出同门，tanh和sigmoid神经元的关系可表示为：
$$
σ(z) = \frac {1+tanh(\frac z 2)}{2}
$$
都是指数系列神经元，不过$tanh$的值域不是$(0,1)$而是$(-1,1)$

同样的，tanh的导数计算也很简单：
$$
tanh(x)' = 1- tanh(x)^2
$$



## 其他激活函数：线性修正神经元

线性修正神经元 ReLU （Recitified Linear Units）可以表示为：
$$
f(z) = max(0,z)
$$
当输入为负时，激活值恒定为0，为正时则激活值为带权输入的线性函数。

相应的，其梯度$f'(z) = 1, z > 0$，而当$z ≤ 0$时，梯度恒为0。

ReLU的好处有：

* Biological plausibility：单边，更符合生物学的激活模型。
* Sparse activation：随机初始化的网络只有一半隐含层处于激活状态。
* Efficient gradient propagation：不像sigmoid出现梯度消失的问题。
* Efficient computation：计算高效，只需要加减乘运算。

通常，在ReLU，Softmax输出层，log-likelihood代价函数的组合下，反向传播方程BP1和BP2计算方式如下：
$$
δ^L = a^L - y \\
δ^l = (W^{l+1})^T δ^{l+1}
$$
如果所得为负数，就直接置为零即可。



## 改变网络组织方式：卷积神经网络（CNN）

CNN的好处都有啥？

* 对于图像平移和图像扭曲有较好的鲁棒性
* 相比全连接网络对内存需求更小。
* 训练更快更容易。

![](images/CNN.png)

卷积神经网络利用了输入的空间结构来构造特征。有几个基本的概念：

* 卷积层（convolution layers）
* 混合层（subsampling/pooling layers）
* 特征映射（feature map）
* 核/过滤器（kernel/filter）
* 局部感受野（local receptive fields）


### 卷积层

卷积层实际完成的工作，是将`feature map`，与`filter/kernel`进行**卷积**，输出的新的`feature map`。

比如这里，输入的FeatureMap为$N×N×D$，使用$H$个Filter来卷积。

对于MNIST问题而言，$N$就是输入图片的长宽：$N×N=28×28$。$D$可以理解为一次输入的图片数目。假设一次输入了$D=10$张图片，那么卷积层的特征映射就是$N×N×D=28×28×10$的一个长方体。

$k$是另外一个参数，是Filter的长宽，令$k×k = 5×5$，步长为$1$，则卷积的输出长宽为$N-k+1 = 24$。

每一个核（Filter/Kernel）和每一个$k×k×D$特征映射的卷积是一个$(N-k+1)×(N-k+1)×1$的平面，所以$N$个核的输出就是$(N-k+1)×(N-k+1)×H$ 的特征映射。



![](images/convolution-process.png)

对于$28×28$的输入图像，$5×5$的局部感受野，如果每次局部感受野移动一个像素的**步长（stride length）**。那么隐藏层中就会有$24×24$个神经元。

### Kernel/Filter：共享权重和偏置

一个核（Kernel）具有共享的权值与偏置参数。即对于第一隐含层的第$j,k$号神经元，其激活值为：
$$
σ \left( b + \sum_{l=0}^4  \sum_{m=0}^4 {w_{l,m} a_{j+l,k+m}}\right)
$$
这意味着第一隐含层的所有神经元检测完全相同的特征， 只不过在输入图像的不同位置。这样的设计使得卷积网络能够很好地适应图像的平移。

每一个特征映射只能检测一种局部特征，为了完成图像识别，往往需要多个特征映射。例如20个特征映射：

![](images/feature-map.png)

这20副图像对应20个不同的特征映射，每个映射由$5×5$的图像表示权重。这些特征映射实际上学习到了一些和空间结构相关的东西。

共享权值和偏置的最大优点在于，相比全连接的网络，它大大减少了网络的参数。

### 混合层（pooling layer）

一个卷积神经网络往往还包含着混合层（pooling layer）。**混合层往往紧接着卷积层使用**，其目的与功能在于简化从卷积层输出的信息。简单地说，混合层将卷积层的每一个特征映射（feature map）进行凝缩。例如以$(2,2)$的`poolsize`对$24×24$的特征映射进行混合，就会得到$12×12$的的浓缩特征。

![](images/pooling.png)

原来的四个像素浓缩成一个像素，常用的混合方法有：平均值混合（average-pooling）最大值混合（max-pooling）与 L2混合（L2-pooling）。

最大值混合取四个像素中值最大的作为输出，$L2$混合取四个像素平方和的平方根，都被广泛使用。

### 卷积混合层集成

![](images/simple-conv.png)

一个卷积层与混合层可以用以下参数表示：

* 过滤器参数：过滤器的个数，局部感受野的宽度，高度，步长。





在全连接的网络中，输入被看做$(784×1)$的排成纵列的神经元，但在一个卷积网络中，输入其实可以看做一个$28 × 28$的方形神经元。之前我们把输入像素连接到一个隐藏神经元层。但是这里，不会把每个输入像素连接到每个隐藏神经元。相反，只是把输入图像进行小的，局部区域的连接。

第一个隐含层中每个神经元会连接到一个输入神经元的一个小区域，例如，一个 $5 × 5$ 的区域，对应于 25 个输入像素。这个输入图像的区域被称为隐藏神经元的**局部感受野（local receptive fields）**。

