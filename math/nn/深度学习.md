# 深度学习(DNN)

[author]: # "Vonng (fengruohang@outlook.com)"
[tags]: # "深度学习，卷积，神经网络，数学"
[category]: # "math/nndl"
[mtime]: #	"2017-05-11 10:00 "
神经网络相关基本知识笔记

---



卷积神经网络利用了输入的空间结构来构造特征。

有三个基本的概念：

* 局部感受野（local receptive fields）
* 共享权值（shared weights）
* 混合（pooling）



## 局部感受野

在全连接的网络中，输入被看做$(784×1)$的排成纵列的神经元，但在一个卷积网络中，输入其实可以看做一个$28 × 28$的方形神经元。之前我们把输入像素连接到一个隐藏神经元层。但是这里，不会把每个输入像素连接到每个隐藏神经元。相反，只是把输入图像进行小的，局部区域的连接。

第一个隐含层中每个神经元会连接到一个输入神经元的一个小区域，例如，一个 $5 × 5​$ 的区域，对应于 25 个输入像素。这个输入图像的区域被称为隐藏神经元的**局部感受野（local receptive fields）**。

![](images/local-receptive-fields.png)

对于$28×28$的输入图像，$5×5$的局部感受野，如果每次局部感受野移动一个像素的**步长（stride length）**。那么隐藏层中就会有$24×24$个神经元。



## 共享权重和偏置

对于第一个隐含层，每个隐藏神经元都具有一个偏置，以及$5×5$共计25个权值。但是这$24×24$个隐藏神经元其实共享同一份权值与偏置参数。即对于第一隐含层的第$j,k$号神经元，其激活值为：
$$
σ \left( b + \sum_{l=0}^4  \sum_{m=0}^4 {w_{l,m} a_{j+l,k+m}}\right)
$$
这意味着第一隐含层的所有神经元检测完全相同的特征， 只不过在输入图像的不同位置。这样的设计使得卷积网络能够很好地适应图像的平移。

因此，从输入层到这种隐藏层的映射，称为一个特征映射（feature map）。而定义特征映射的权重称为**共享权重（shared weight）**，**共享偏置（shared bias）**同理。

共享权重和偏置有时候也被称为一个**卷积核（kernel）**或**滤波器（filter）**

每一个特征映射只能检测一种局部特征，为了完成图像识别，往往需要多个特征映射。例如20个特征映射：

![](images/feature-map.png)

这20副图像对应20个不同的特征映射，每个映射由$5×5$的图像表示权重。这些特征映射实际上学习到了一些和空间结构相关的东西。

共享权值和偏置的最大优点在于，相比全连接的网络，它大大减少了网络的参数。



## 混合层（pooling layer）

一个卷积神经网络往往还包含着混合层（pooling layer）。混合层往往紧接着卷积层使用，其目的与功能在于简化从卷积层输出的信息。

![](images/simple-conv.png)